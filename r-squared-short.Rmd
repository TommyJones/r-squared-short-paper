---
title: "A New (Old) Goodness of Fit Metric for Multidimensional Outcomes"
author: 
  - Tommy Jones^[Dept. of Computational and Data Sciences, George Mason University]
  - Mark Meyer^[Dept. of Mathematics and Statistics, Georgetown University]
date: "`r Sys.Date()`"
bibliography: [r2.bib, manual.bib]
header-includes:
    - \usepackage{setspace}\doublespacing
    - \usepackage[utf8]{inputenc}
    - \usepackage[T1]{fontenc}
output:
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)

options(tinytex.verbose = TRUE)
```

# Introduction
The coefficient of determination---$R^2$---is the most popular goodness of fit metric for linear models. It has appealing properties such as a lower bound of zero, an upper bound of one, and interpretable as the proportion of varience in the outcome accounted for by the model. $R^2$'s appeal is such that nearly all statistical software reports $R^2$ by default when fitting linear models.

The standard, and model free, definition of $R^2$ is

\begin{equation}
  \label{eqn:r2def1}
    R^2 
      \equiv 1 - \frac{SS_{resid.}}{SS_{tot.}}
\end{equation} 

or equivalently

\begin{equation}
  \label{eqn:r2def2}
    R^2 
      \equiv 1 - \frac{\sum_{i=1}^N{(\hat{y}_i-y_i)^2}}{\sum_{i=1}^N{(y_i-\bar{y})^2}}
\end{equation} 

In (\ref{eqn:r2def1}) $SS_{resid.}$ is called the _residual sum of squares_ and $SS_{tot.}$ is called the _total sum of squares_. And in (\ref{eqn:r2def2}) $\bar{y}$ is the sample mean and $\hat{y}_i$ is the value predicted for observation $y_i$. 

The definitions in In (\ref{eqn:r2def1}) and (\ref{eqn:r2def2}) are "model free" but they are alegebraically equivalent to

\begin{equation}
  \label{eqn:r2var}
    R^2 
      = \frac{\frac{1}{N - 1} \sum_{i=1}^N (\hat{y}_i - \bar{y})^2}{\frac{1}{N - 1} \sum_{i=1}^N (y_i - \bar{y})^2}
\end{equation} 

For a linear model, equation (\ref{eqn:r2var}) is the proportion of variance in $y$ accounted for by the model. Researchers have used and extended the variance relationship in (\ref{eqn:r2var}) to develop $R^2$ metrics for a range of models such as logistic regression [@hu2006], mixed models [@piepho2019], Bayesian regression models [@gelman2018r2], and more.

To our knowledge our research is the first time anyone has proposed a variation of $R^2$ for models predicting an outcome in multiple dimensions---where each $\boldsymbol{y}_i$ is a vector and $\boldsymbol{Y}$ is a matrix. Multidimensional outcomes occur in settings such as [need good examples here... simultaneous equations? neural networks? topic models?] Our $R^2$ relies on a geometric interpretation of (\ref{eqn:r2def1}). As a result, it is also model free. 

# A Geometric Interpretation of $R^2$
The numerator and denominator in (\ref{eqn:r2def2}) may be viewed as sums of squared Euclidean distances in $\mathbb{R}_1$. Specifically, $SS_{tot.}$ is the total squared-Euclidean distance from each $y_i$ to the mean outcome, $\bar{y}$. Then $SS_{resid.}$ is the total squared-Euclidean distance from each $y_i$ to its predicted value under the model, $\hat{y}_i$.

As a reminder, for any two points $\boldsymbol{p}, \boldsymbol{q} \in \mathbb{R}_M$, the Euclidean distance between $\boldsymbol{p}$ and $\boldsymbol{q}$ is 

\begin{equation}
  \label{eqn:euclidean}
	d(\boldsymbol{p},\boldsymbol{q}) = \sqrt{\sum_{i=1}^M{(p_i - q_i)^2}}
\end{equation}

Using the notation from (\ref{eqn:euclidean}), we can rewrite (\ref{eqn:r2def1}) and (\ref{eqn:r2def2}) as

\begin{equation}
  \label{eqn:r2euclidean}
  R^2
    = 1 - \frac{\sum_{i=1}^N d(\boldsymbol{y}_i, \hat{\boldsymbol{y}}_i)^2}{\sum_{i=1}^N d(\boldsymbol{y}_i, \bar{\boldsymbol{y}})^2}
\end{equation}

Equation (\ref{eqn:r2euclidean}) extends $R^2$ to cover outcomes in $\mathbb{R}_M$ while preserving the calculation from (\ref{eqn:r2def2}) for outcomes in $\mathbb{R}_1$. 

Fig. 1 visualizes the geometric interpretation of $R^2$ for outcomes in $\mathbb{R}_2$. The left image represents $SS_{tot.}$: the red dots are data points ($\boldsymbol{y}_i$); the black dot is the vector of means ($\bar{\boldsymbol{y}}$); the line segments represent the Euclidean distance from each $\boldsymbol{y}_i$ to $\bar{\boldsymbol{y}}$. $SS_{tot.}$ is obtained by squaring the length of each line segment and then adding the squared segments together. The right image represents $SS_{resid.}$: the blue dots are the fitted values ($\hat{\boldsymbol{y}}_i$); the line segments represent the Euclidean distance from each $\hat{\boldsymbol{y}}_i$ to its corresponding $\boldsymbol{y}_i$. $SS_{resid.}$ is obtained by squaring the length of each line segment and then adding the squared segments together.

```{r geometric_graphic, fig.cap = "Visualizing the geometric interpretation of R-squared. Sum up the squared length of each line segment for the total (left) or residual (right) sums of squares. This figure corresponds to an R-squared of 0.87", fig.asp = .55}
set.seed("8675309")

# Generate sample data poings
mymat <- data.frame(y1=rnorm(n=5, mean=5, sd=3), y2=rnorm(n=5, mean=3, sd=5))

# Generate "predicted" values by adding jitter
mymat$y1hat <- jitter(mymat$y1, factor=10)
mymat$y2hat <- jitter(mymat$y2, factor=10)

# Calculate mean point
mymat$y1bar <- mean(mymat$y1)
mymat$y2bar <- mean(mymat$y2)

# Calculate sums of squares
ssres <- sapply(1:nrow(mymat), function(j){
    (mymat$y1[ j ] -mymat$y1hat[ j ])^2 + (mymat$y2[ j ] - mymat$y2hat[ j ])^2
})

sst <- sapply(1:nrow(mymat), function(j){
    (mymat$y1[ j ] -mymat$y1bar[ j ])^2 + (mymat$y2[ j ] - mymat$y2bar[ j ])^2
})

ssm <- sapply(1:nrow(mymat), function(j){
    (mymat$y1bar[ j ] -mymat$y1hat[ j ])^2 + (mymat$y2bar[ j ] - mymat$y2hat[ j ])^2
})

# plot those suckers
par(mfrow = c(1, 2))

plot(mymat[ , c("y1", "y2" ) ], yaxt="n", xaxt="n", pch=19, col=rgb(1,0,0,0.5), 
     ylim=c(min(c(mymat$y2, mymat$y2hat)),max(mymat$y2, mymat$y2hat)),
     xlim=c(min(c(mymat$y1, mymat$y1hat)),max(mymat$y1, mymat$y1hat)),
     main="Total Sum of Squares")
points(mymat[ , c("y1bar", "y2bar") ], pch=19)
for(j in 1:nrow(mymat)){
    lines(c(mymat$y1[ j ],mymat$y1bar[ j ]), c(mymat$y2[ j ],mymat$y2bar[ j ]))
}


plot(mymat[ , c("y1", "y2" ) ], yaxt="n", xaxt="n", pch=19, col=rgb(1,0,0,0.5), 
     ylim=c(min(c(mymat$y2, mymat$y2hat)),max(mymat$y2, mymat$y2hat)),
     xlim=c(min(c(mymat$y1, mymat$y1hat)),max(mymat$y1, mymat$y1hat)),
     main="Residual Sum of Squares")
points(mymat[ , c("y1hat", "y2hat") ], pch=17, col=rgb(0,0,1,0.5))
for(j in 1:nrow(mymat)){
    lines(c(mymat$y1[ j ],mymat$y1hat[ j ]), c(mymat$y2[ j ],mymat$y2hat[ j ]))
}

# 1 - sum(ssres)/sum(sst)
```


# Properties 
You can view the $R^2$ in (\ref{eqn:r2euclidean}) as the proportion of total squared distance from each observation to its mean that is accounted for by the model, similar to the "proportion of explained variance" interpretation. 

This $R^2$ is maximized at 1---representing perfect predictions. Unlike traditional $R^2$, it does not have a lower bound, as can be the case in certain circumstances with tradtional $R^2$ [@barten1987] [@windmeijer1997]. Losing the lower bound of zero does not negatively impact interpretation. When $R^2 = 0$, then $SS_{resid.} = SS_{tot.}$; the model is no better than guessing the mean outcome. So negative values of $R^2$ mean the model is _worse_ than guessing the mean outcome for every observation. Its interpretation remains straightforward.

The $R^2$ given in (\ref{eqn:r2euclidean}) is sensitive to the scale of the dimensions of $\boldsymbol{Y}$. This is a common property of Euclidean distance, exacerbated by squaring. Possible mitigating steps may be standardizing the columns of $\boldsymbol{Y}$ before modeling or standardinzing both the columns of $\boldsymbol{Y}$ and $\hat{\boldsymbol{Y}}$ after modeling. When one standardizes may lead to different results, an exploration of which is out of scope for this note.

# Discussion
Viewing $R^2$ as a ratio of distances motivates a new direction for extended definitions of $R^2$. Researchers may wish to explore $R^2$ calculations based other distance measures more appropriate for other settings. For example, if one's model estimates probabilities perhaps Hellinger distance [@hellinger1909new] would be more appropriate. If scale between outcome variables is of concern, maybe an $R^2$ leveraging Mahalanobis distance [@mahalanobis] has advantage over variable normalization as discussed above. 

We have shown that $R^2$ can be calculated for multivariate outcomes from a geometric interpretation of (\ref{eqn:r2def1}). Since we use the standard definition of $R^2$, this approach does not alter existing issues motivating alternate definitions of $R^2$ for, e.g., Bayesian models or nonlinear models. It is worth noting that scale differences in outcome variables can swamp the calculation. We leave an exploration of remediation strategies to future work. We have implemented the $R^2$ metric in (\ref{eqn:r2euclidean}) in a package for the R programming language called `mvrsquared` [@mvrsquared].

\newpage{}
# References